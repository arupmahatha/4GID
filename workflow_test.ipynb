{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Generation and Analysis Workflow Test\n",
    "\n",
    "This notebook demonstrates the complete workflow using multiple LLMs:\n",
    "1. Generating SQL using Mistral and SQLCoder\n",
    "2. Extracting entities from the SQL\n",
    "3. Matching values in the SQL\n",
    "4. Refining the generated SQL\n",
    "5. Executing the refined SQL\n",
    "6. Analyzing the results\n",
    "\n",
    "Each step uses Mistral and SQLCoder for generation, with Llama2 as the decision maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config.llm_call import generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order the type of institution in decreasing order of gender 'M', ascending order of gender 'F' and descending order of gender 'O' students\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find the name and email of a learner named David Gardner or who has the email allisonthomson@example.com\",\n",
    "    \"Find the number of students form the institution with l4g_code L4G0003\",\n",
    "    \"Show me all learners who have completed their course in the last 6 months\",\n",
    "    \"Analyze the distribution of students by gender, age group, and enrollment year. Skip any 'O' values in data\",\n",
    "    \"What is the difference in count between gender 'M' and 'F' students\",\n",
    "    \"Order the type of institution in decreasing order of gender 'M', ascending order of gender 'F' and descending order of gender 'O' students\"\n",
    "]\n",
    "\n",
    "# Select which query to test\n",
    "user_query = test_queries[5]\n",
    "\n",
    "print(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQL Generation Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating SQL with Model 1...\n",
      "\n",
      "Generated SQL: SELECT \n",
      "    i.type AS institution_type,\n",
      "    COUNT(CASE WHEN l.gender = 'M' THEN l.id END) AS male_students_count,\n",
      "    COUNT(CASE WHEN l.gender = 'F' THEN l.id END) AS female_students_count,\n",
      "    COUNT(CASE WHEN l.gender = 'O' THEN l.id END) AS other_students_count\n",
      "FROM \n",
      "    Institution i\n",
      "JOIN \n",
      "    Learner_Education le ON i.id = le.institution_code\n",
      "JOIN \n",
      "    Learner l ON le.learner_code = l.id\n",
      "GROUP BY \n",
      "    i.type\n",
      "ORDER BY \n",
      "    male_students_count DESC,\n",
      "    female_students_count ASC,\n",
      "    other_students_count DESC;\n",
      "\n",
      "Generating SQL with Model 2...\n",
      "Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Generated SQL: Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Schema Info: Country (id, name)\n",
      "State (id, name, code, country_code)\n",
      "District (id, name, code, state_code)\n",
      "Institution (id, name, short_name, aicte_code, eamcet_code, l4g_code, l4g_group_code, type, address, website, latlong, district_code)\n",
      "Degree (id, name, short_name)\n",
      "Branch (id, name, short_name, degree_code)\n",
      "Department (id, name, type)\n",
      "Designation (id, name, type, priority)\n",
      "Knowledge_Partner (id, name, address, website, info)\n",
      "Course (id, name, info, knowledge_partner_code)\n",
      "Module (id, name, info, module_sequence_number, theory_practical, duration_minutes, course_code)\n",
      "Specialization (id, name, info, knowledge_partner_code)\n",
      "Specialization_Course (id, course_sequence_number, course_code, specialization_code)\n",
      "Program (id, name, info, knowledge_partner_code)\n",
      "Program_Specialization (id, program_code, specialization_code)\n",
      "Program_Requirement (id, name, is_mandatory, program_code)\n",
      "Learner (id, name, email, mobile, gender, date_of_birth, aadhaar_number)\n",
      "Learner_Education (id, rollno, year_of_joining, year_of_graduation, learner_code, institution_code, branch_code)\n",
      "Learner_Employment (id, empid, year_of_joining, learner_code, institution_code, department_code, designation_code)\n",
      "Learner_Program_Requirement (id, learner_code, program_requirement_code, value)\n",
      "\n",
      "Validator chose Model 1's query\n"
     ]
    }
   ],
   "source": [
    "from engine.generator import SQLGenerator\n",
    "\n",
    "def test_generator(model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"deepseek-chat\"):\n",
    "    \"\"\"Test SQL generation functionality with multiple LLMs\"\"\"\n",
    "    generator = SQLGenerator()\n",
    "    \n",
    "    try:\n",
    "        # Generate SQL using Model 1\n",
    "        print(\"\\nGenerating SQL with Model 1...\")\n",
    "        model1_results = generator.main_generator(user_query, llm_model=model1)\n",
    "        print(\"\\nGenerated SQL:\", model1_results['generated_sql'])\n",
    "        \n",
    "        # Generate SQL using Model 2\n",
    "        print(\"\\nGenerating SQL with Model 2...\")\n",
    "        model2_results = generator.main_generator(user_query, llm_model=model2)\n",
    "        print(\"\\nGenerated SQL:\", model2_results['generated_sql'])\n",
    "\n",
    "        print(\"\\nSchema Info:\", model1_results['formatted_metadata'])\n",
    "        \n",
    "        # Use Validator to decide which SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two Generated SQLs for the given query and choose the better one:\n",
    "        \n",
    "        Input Query: {user_query}\n",
    "        \n",
    "        Schema Info: {model1_results['formatted_metadata']}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Generated SQL: {model1_results['generated_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Generated SQL: {model2_results['generated_sql']}\n",
    "        \n",
    "        Consider the input query, schema info, and the generated SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which output is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen query\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's query\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's query\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['generated_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "generated_sql = test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entity Extraction Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting entities with Model 1...\n",
      "\n",
      "Extracted Entities:\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Private University-State\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Autonomous College\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Private University-Deemed to be\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Unknown\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Public University\n",
      "---\n",
      "Table: Institution\n",
      "Column: type\n",
      "Value: Affiliated College\n",
      "---\n",
      "Table: Learner\n",
      "Column: gender\n",
      "Value: M\n",
      "---\n",
      "Table: Learner\n",
      "Column: gender\n",
      "Value: F\n",
      "---\n",
      "Table: Learner\n",
      "Column: gender\n",
      "Value: O\n",
      "\n",
      "Extracting entities with Model 2...\n",
      "Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Extracted Entities:\n",
      "\n",
      "Validator chose Model 1's entities\n"
     ]
    }
   ],
   "source": [
    "from engine.entity_extractor import EntityExtractor\n",
    "\n",
    "def test_entity_extractor(sql_query, model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"deepseek-chat\"):\n",
    "    \"\"\"Test entity extraction functionality with multiple LLMs\"\"\"\n",
    "    extractor = EntityExtractor()\n",
    "    \n",
    "    try:\n",
    "        # Extract entities using Model 1\n",
    "        print(\"\\nExtracting entities with Model 1...\")\n",
    "        model1_results = extractor.main_entity_extractor(sql_query, llm_model=model1)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model1_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Extract entities using Model 2\n",
    "        print(\"\\nExtracting entities with Model 2...\")\n",
    "        model2_results = extractor.main_entity_extractor(sql_query, llm_model=model2)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model2_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Use Validator to decide which entities to use\n",
    "        decision_prompt = f\"\"\"Compare these two sets of extracted entities for the input SQL and choose the better one:\n",
    "        \n",
    "        Input SQL: {sql_query}\n",
    "        \n",
    "        Entities 1 (Model 1):\n",
    "        Extracted Entities: {model1_results}\n",
    "        \n",
    "        Entities 2 (Model 2):\n",
    "        Extracted Entities: {model2_results}\n",
    "        \n",
    "        Consider the input SQL and both the extracted entities when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which set is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen entities\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's entities\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's entities\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if generated_sql:\n",
    "    extracted_entities = test_entity_extractor(generated_sql)\n",
    "else:\n",
    "    print(\"Skipping entity extraction as no SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Matching Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Mappings:\n",
      "---\n",
      "Original: 'Private University-State'\n",
      "Matched: 'Private University-State'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'Autonomous College'\n",
      "Matched: 'Autonomous College'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'Private University-Deemed to be'\n",
      "Matched: 'Private University-Deemed to be'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'Unknown'\n",
      "Matched: 'Unknown'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'Public University'\n",
      "Matched: 'Public University'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'Affiliated College'\n",
      "Matched: 'Affiliated College'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'M'\n",
      "Matched: 'M'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'F'\n",
      "Matched: 'F'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'O'\n",
      "Matched: 'O'\n",
      "Score: 100\n"
     ]
    }
   ],
   "source": [
    "from engine.value_matcher import ValueMatcher\n",
    "\n",
    "def test_value_matcher(extracted_entities):\n",
    "    \"\"\"Test value matching functionality\"\"\"\n",
    "    matcher = ValueMatcher()\n",
    "    \n",
    "    try:\n",
    "        value_mappings = []\n",
    "        for entity in extracted_entities:\n",
    "            match = matcher.main_value_matcher(entity)\n",
    "            value_mappings.extend(match)\n",
    "        \n",
    "        print(\"Value Mappings:\")\n",
    "        for mapping in value_mappings:\n",
    "            print(\"---\")\n",
    "            print(f\"Original: '{mapping['original_value']}'\")\n",
    "            print(f\"Matched: '{mapping['matched_value']}'\")\n",
    "            print(f\"Score: {mapping['score']}\")\n",
    "        \n",
    "        return value_mappings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if extracted_entities:\n",
    "    value_mappings = test_value_matcher(extracted_entities)\n",
    "else:\n",
    "    print(\"Skipping value matching as no entities were extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL Refinement Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refining SQL with Model 1...\n",
      "\n",
      "Refined SQL: SELECT \n",
      "    i.type AS institution_type,\n",
      "    COUNT(CASE WHEN l.gender = 'M' THEN l.id END) AS male_students_count,\n",
      "    COUNT(CASE WHEN l.gender = 'F' THEN l.id END) AS female_students_count,\n",
      "    COUNT(CASE WHEN l.gender = 'O' THEN l.id END) AS other_students_count\n",
      "FROM \n",
      "    Institution i\n",
      "JOIN \n",
      "    Learner_Education le ON i.id = le.institution_code\n",
      "JOIN \n",
      "    Learner l ON le.learner_code = l.id\n",
      "GROUP BY \n",
      "    i.type\n",
      "ORDER BY \n",
      "    male_students_count DESC,\n",
      "    female_students_count ASC,\n",
      "    other_students_count DESC;\n",
      "\n",
      "Refining SQL with Model 2...\n",
      "Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Refined SQL: Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Validator chose Model 1's refined SQL\n"
     ]
    }
   ],
   "source": [
    "from engine.refiner import SQLRefiner\n",
    "\n",
    "def test_refiner(sql_query, value_mappings, model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"deepseek-chat\"):\n",
    "    \"\"\"Test SQL refinement functionality with multiple LLMs\"\"\"\n",
    "    refiner = SQLRefiner()\n",
    "    \n",
    "    try:\n",
    "        # Refine SQL using Model 1\n",
    "        print(\"\\nRefining SQL with Model 1...\")\n",
    "        model1_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model1)\n",
    "        print(\"\\nRefined SQL:\", model1_results['refined_sql'])\n",
    "        \n",
    "        # Refine SQL using Model 2\n",
    "        print(\"\\nRefining SQL with Model 2...\")\n",
    "        model2_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model2)\n",
    "        print(\"\\nRefined SQL:\", model2_results['refined_sql'])\n",
    "        \n",
    "        # Use Validator to decide which refined SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two refined SQL queries for the given original SQL and value mappings, and choose the better one:\n",
    "        \n",
    "        Original SQL: {sql_query}\n",
    "        Value Mappings: {value_mappings}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Refined SQL: {model1_results['refined_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Refined SQL: {model2_results['refined_sql']}\n",
    "        \n",
    "        Consider the original sql, value mappings and the refined SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which query is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen refined SQL\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's refined SQL\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's refined SQL\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['refined_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if value_mappings:\n",
    "    refined_sql = test_refiner(generated_sql, value_mappings)\n",
    "else:\n",
    "    print(\"Skipping refinement as no value mappings were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Execution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Found 6 rows\n",
      "\n",
      "Formatted Results:\n",
      "institution_type                | male_students_count | female_students_count | other_students_count\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Private University-State        | 178                 | 168                   | 127                 \n",
      "Autonomous College              | 113                 | 129                   | 85                  \n",
      "Private University-Deemed to be | 84                  | 95                    | 80                  \n",
      "Unknown                         | 54                  | 53                    | 46                  \n",
      "Public University               | 51                  | 49                    | 49                  \n",
      "Affiliated College              | 47                  | 55                    | 37                  \n",
      "\n",
      "Raw Results:\n",
      "[{'institution_type': 'Private University-State', 'male_students_count': 178, 'female_students_count': 168, 'other_students_count': 127}, {'institution_type': 'Autonomous College', 'male_students_count': 113, 'female_students_count': 129, 'other_students_count': 85}, {'institution_type': 'Private University-Deemed to be', 'male_students_count': 84, 'female_students_count': 95, 'other_students_count': 80}, {'institution_type': 'Unknown', 'male_students_count': 54, 'female_students_count': 53, 'other_students_count': 46}, {'institution_type': 'Public University', 'male_students_count': 51, 'female_students_count': 49, 'other_students_count': 49}, {'institution_type': 'Affiliated College', 'male_students_count': 47, 'female_students_count': 55, 'other_students_count': 37}]\n"
     ]
    }
   ],
   "source": [
    "from engine.executor import SQLExecutor\n",
    "\n",
    "def test_executor(sql_query):\n",
    "    \"\"\"Test SQLExecutor functionality\"\"\"\n",
    "    executor = SQLExecutor()\n",
    "    \n",
    "    success, results, formatted_results, error = executor.main_executor(sql_query)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nSuccess! Found {len(results)} rows\")\n",
    "        print(\"\\nFormatted Results:\")\n",
    "        print(formatted_results)\n",
    "        print(\"\\nRaw Results:\")\n",
    "        print(results)\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"\\nFailed: {error}\")\n",
    "        return None\n",
    "\n",
    "if refined_sql:\n",
    "    execution_results = test_executor(refined_sql)\n",
    "else:\n",
    "    print(\"Skipping execution as no refined SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results with DeepSeek...\n",
      "\n",
      "Analysis: Here's a comprehensive analysis of the institution data based on the query requirements:\n",
      "\n",
      "1. Key Findings and Patterns:\n",
      "- Private University-State dominates in all categories with the highest counts (178M, 168F, 127O)\n",
      "- Male students consistently outnumber female students in Private University-State and Public University\n",
      "- Female students outnumber males in Autonomous College, Private University-Deemed, and Affiliated College\n",
      "- The \"Unknown\" institution type shows near gender parity (54M vs 53F)\n",
      "- Other gender (O) representation follows similar proportions to main genders across institutions\n",
      "\n",
      "2. Notable Relationships:\n",
      "- Strong positive correlation between institution size and gender counts (larger institutions have higher numbers across all genders)\n",
      "- Inverse relationship between male and female counts in most institutions (when M is high, F tends to be lower relative to M, and vice versa)\n",
      "- Other gender counts consistently represent about 25-30% of the primary gender counts\n",
      "\n",
      "3. Important Trends/Anomalies:\n",
      "- Autonomous College shows unusual female dominance (129F vs 113M) compared to similar-sized Private University-Deemed\n",
      "- Public University shows perfect gender balance in other category (49O for both M/F)\n",
      "- Affiliated College has the smallest enrollment but highest female ratio (55F vs 47M)\n",
      "- Private institutions collectively enroll 3x more students than public institutions\n",
      "\n",
      "4. Actionable Insights and Recommendations:\n",
      "- Recruitment Focus: Target Private University-State for largest potential student pools across all genders\n",
      "- Gender Initiatives: Investigate successful female recruitment at Autonomous College for replication\n",
      "- Diversity Programs: Expand \"other gender\" support at Private University-State which has highest absolute numbers\n",
      "- Research Opportunity: Study the Unknown category's near-perfect gender balance for best practices\n",
      "- Resource Allocation: Prioritize Private University-State for student services given its disproportionate enrollment\n",
      "\n",
      "The data suggests institution type strongly correlates with gender distribution patterns, with private institutions generally having larger gender disparities than public/unknown types. This could inform targeted recruitment strategies and support service planning.\n"
     ]
    }
   ],
   "source": [
    "from engine.analyzer import SQLAnalyzer\n",
    "\n",
    "def test_analyzer(query, results):\n",
    "    \"\"\"Test SQLAnalyzer functionality with DeepSeek\"\"\"\n",
    "    analyzer = SQLAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Analyze results using DeepSeek\n",
    "        print(\"\\nAnalyzing results with DeepSeek...\")\n",
    "        mistral_results = analyzer.main_analyzer(query, results, llm_model=\"deepseek-chat\")\n",
    "        print(\"\\nAnalysis:\", mistral_results['analysis'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if execution_results:\n",
    "    test_analyzer(user_query, execution_results)\n",
    "else:\n",
    "    print(\"Skipping analysis as no execution results were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our last chat involved analyzing institution data by gender distribution and providing insights based on SQL query results.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt to test the conversation history\n",
    "prompt = \"What was our last chat about? Explain in a single sentence.\"\n",
    "response = generate_text(prompt, model=\"deepseek-chat\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
