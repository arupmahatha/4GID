{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Generation and Analysis Workflow Test\n",
    "\n",
    "This notebook demonstrates the complete workflow using multiple LLMs:\n",
    "1. Generating SQL using Mistral and SQLCoder\n",
    "2. Extracting entities from the SQL\n",
    "3. Matching values in the SQL\n",
    "4. Refining the generated SQL\n",
    "5. Executing the refined SQL\n",
    "6. Analyzing the results\n",
    "\n",
    "Each step uses Mistral and SQLCoder for generation, with Llama2 as the decision maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config.llm_call import generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find the name and email of a learner named David Gardner or who has the email allisonthomson@example.com\",\n",
    "    \"Find the number of students form the institution with l4g_code L4G0003\",\n",
    "    \"Show me all learners who have completed their course in the last 6 months\"\n",
    "]\n",
    "\n",
    "# Select which query to test\n",
    "user_query = test_queries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQL Generation Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating SQL with Model 1...\n",
      "\n",
      "Generated SQL: SELECT name, email\n",
      "FROM Learner\n",
      "WHERE name = 'David Gardner'\n",
      "OR email = 'allisonthomson@example.com';\n",
      "\n",
      "Generating SQL with Model 2...\n",
      "\n",
      "Generated SQL: SELECT name, email\n",
      "FROM Learner\n",
      "WHERE name = 'David Gardner'\n",
      "OR email = 'allisonthomson@example.com';\n",
      "\n",
      "Schema Info: Country (id, name)\n",
      "State (id, name, code, country_code)\n",
      "District (id, name, code, state_code)\n",
      "Institution (id, name, short_name, aicte_code, eamcet_code, l4g_code, l4g_group_code, type, address, website, latlong, district_code)\n",
      "Degree (id, name, short_name)\n",
      "Branch (id, name, short_name, degree_code)\n",
      "Department (id, name, type)\n",
      "Designation (id, name, type, priority)\n",
      "Knowledge_Partner (id, name, address, website, info)\n",
      "Course (id, name, info, knowledge_partner_code)\n",
      "Module (id, name, info, module_sequence_number, theory_practical, duration_minutes, course_code)\n",
      "Specialization (id, name, info, knowledge_partner_code)\n",
      "Specialization_Course (id, course_sequence_number, course_code, specialization_code)\n",
      "Program (id, name, info, knowledge_partner_code)\n",
      "Program_Specialization (id, program_code, specialization_code)\n",
      "Program_Requirement (id, name, is_mandatory, program_code)\n",
      "Learner (id, name, email, mobile, gender, date_of_birth, aadhaar_number)\n",
      "Learner_Education (id, rollno, year_of_joining, year_of_graduation, learner_code, institution_code, branch_code)\n",
      "Learner_Employment (id, empid, year_of_joining, learner_code, institution_code, department_code, designation_code)\n",
      "Learner_Program_Requirement (id, learner_code, program_requirement_code, value)\n",
      "\n",
      "Validator chose Model 1's query\n"
     ]
    }
   ],
   "source": [
    "from engine.generator import SQLGenerator\n",
    "\n",
    "def test_generator(model1=\"mistralai/Mistral-7B-Instruct-v0.3\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"Test SQL generation functionality with multiple LLMs\"\"\"\n",
    "    generator = SQLGenerator()\n",
    "    \n",
    "    try:\n",
    "        # Generate SQL using Model 1\n",
    "        print(\"\\nGenerating SQL with Model 1...\")\n",
    "        model1_results = generator.main_generator(user_query, llm_model=model1)\n",
    "        print(\"\\nGenerated SQL:\", model1_results['generated_sql'])\n",
    "        \n",
    "        # Generate SQL using Model 2\n",
    "        print(\"\\nGenerating SQL with Model 2...\")\n",
    "        model2_results = generator.main_generator(user_query, llm_model=model2)\n",
    "        print(\"\\nGenerated SQL:\", model2_results['generated_sql'])\n",
    "\n",
    "        print(\"\\nSchema Info:\", model1_results['formatted_metadata'])\n",
    "        \n",
    "        # Use Validator to decide which SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two Generated SQLs for the given query and choose the better one:\n",
    "        \n",
    "        Input Query: {user_query}\n",
    "        \n",
    "        Schema Info: {model1_results['formatted_metadata']}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Generated SQL: {model1_results['generated_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Generated SQL: {model2_results['generated_sql']}\n",
    "        \n",
    "        Consider the input query, schema info, and the generated SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which output is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen query\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's query\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's query\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['generated_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "generated_sql = test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entity Extraction Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting entities with Model 1...\n",
      "\n",
      "Extracted Entities:\n",
      "---\n",
      "Table: Learner\n",
      "Column: name\n",
      "Value: David Gardner\n",
      "---\n",
      "Table: Learner\n",
      "Column: email\n",
      "Value: allisonthomson@example.com\n",
      "\n",
      "Extracting entities with Model 2...\n",
      "\n",
      "Extracted Entities:\n",
      "---\n",
      "Table: Learner\n",
      "Column: name\n",
      "Value: David Gardner\n",
      "---\n",
      "Table: Learner\n",
      "Column: email\n",
      "Value: allisonthomson@example.com\n",
      "\n",
      "Validator chose Model 1's entities\n"
     ]
    }
   ],
   "source": [
    "from engine.entity_extractor import EntityExtractor\n",
    "\n",
    "def test_entity_extractor(sql_query, model1=\"mistralai/Mistral-7B-Instruct-v0.3\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"Test entity extraction functionality with multiple LLMs\"\"\"\n",
    "    extractor = EntityExtractor()\n",
    "    \n",
    "    try:\n",
    "        # Extract entities using Model 1\n",
    "        print(\"\\nExtracting entities with Model 1...\")\n",
    "        model1_results = extractor.main_entity_extractor(sql_query, llm_model=model1)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model1_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Extract entities using Model 2\n",
    "        print(\"\\nExtracting entities with Model 2...\")\n",
    "        model2_results = extractor.main_entity_extractor(sql_query, llm_model=model2)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model2_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Use Validator to decide which entities to use\n",
    "        decision_prompt = f\"\"\"Compare these two sets of extracted entities for the input SQL and choose the better one:\n",
    "        \n",
    "        Input SQL: {sql_query}\n",
    "        \n",
    "        Entities 1 (Model 1):\n",
    "        Extracted Entities: {model1_results}\n",
    "        \n",
    "        Entities 2 (Model 2):\n",
    "        Extracted Entities: {model2_results}\n",
    "        \n",
    "        Consider the input SQL and both the extracted entities when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which set is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen entities\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's entities\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's entities\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if generated_sql:\n",
    "    extracted_entities = test_entity_extractor(generated_sql)\n",
    "else:\n",
    "    print(\"Skipping entity extraction as no SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Matching Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Mappings:\n",
      "---\n",
      "Original: 'David Gardner'\n",
      "Matched: 'David Garner'\n",
      "Score: 96\n",
      "---\n",
      "Original: 'allisonthomson@example.com'\n",
      "Matched: 'allisonthompson@example.com'\n",
      "Score: 98\n"
     ]
    }
   ],
   "source": [
    "from engine.value_matcher import ValueMatcher\n",
    "\n",
    "def test_value_matcher(extracted_entities):\n",
    "    \"\"\"Test value matching functionality\"\"\"\n",
    "    matcher = ValueMatcher()\n",
    "    \n",
    "    try:\n",
    "        value_mappings = []\n",
    "        for entity in extracted_entities:\n",
    "            match = matcher.main_value_matcher(entity)\n",
    "            value_mappings.extend(match)\n",
    "        \n",
    "        print(\"Value Mappings:\")\n",
    "        for mapping in value_mappings:\n",
    "            print(\"---\")\n",
    "            print(f\"Original: '{mapping['original_value']}'\")\n",
    "            print(f\"Matched: '{mapping['matched_value']}'\")\n",
    "            print(f\"Score: {mapping['score']}\")\n",
    "        \n",
    "        return value_mappings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if extracted_entities:\n",
    "    value_mappings = test_value_matcher(extracted_entities)\n",
    "else:\n",
    "    print(\"Skipping value matching as no entities were extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL Refinement Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refining SQL with Model 1...\n",
      "\n",
      "Refined SQL: SELECT name, email\n",
      "FROM Learner\n",
      "WHERE name = 'David Garner'\n",
      "OR email = 'allisonthompson@example.com';\n",
      "\n",
      "Refining SQL with Model 2...\n",
      "\n",
      "Refined SQL: SELECT name, email\n",
      "FROM Learner\n",
      "WHERE name = 'David Garner'\n",
      "OR email = 'allisonthompson@example.com';\n",
      "\n",
      "Validator chose Model 1's refined SQL\n"
     ]
    }
   ],
   "source": [
    "from engine.refiner import SQLRefiner\n",
    "\n",
    "def test_refiner(sql_query, value_mappings, model1=\"mistralai/Mistral-7B-Instruct-v0.3\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"Test SQL refinement functionality with multiple LLMs\"\"\"\n",
    "    refiner = SQLRefiner()\n",
    "    \n",
    "    try:\n",
    "        # Refine SQL using Model 1\n",
    "        print(\"\\nRefining SQL with Model 1...\")\n",
    "        model1_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model1)\n",
    "        print(\"\\nRefined SQL:\", model1_results['refined_sql'])\n",
    "        \n",
    "        # Refine SQL using Model 2\n",
    "        print(\"\\nRefining SQL with Model 2...\")\n",
    "        model2_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model2)\n",
    "        print(\"\\nRefined SQL:\", model2_results['refined_sql'])\n",
    "        \n",
    "        # Use Validator to decide which refined SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two refined SQL queries for the given original SQL and value mappings, and choose the better one:\n",
    "        \n",
    "        Original SQL: {sql_query}\n",
    "        Value Mappings: {value_mappings}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Refined SQL: {model1_results['refined_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Refined SQL: {model2_results['refined_sql']}\n",
    "        \n",
    "        Consider the original sql, value mappings and the refined SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which query is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen refined SQL\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's refined SQL\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's refined SQL\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['refined_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if value_mappings:\n",
    "    refined_sql = test_refiner(generated_sql, value_mappings)\n",
    "else:\n",
    "    print(\"Skipping refinement as no value mappings were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Execution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Found 1 rows\n",
      "\n",
      "Formatted Results:\n",
      "name         | email                      \n",
      "------------------------------------------\n",
      "David Garner | allisonthompson@example.com\n",
      "\n",
      "Raw Results:\n",
      "[{'name': 'David Garner', 'email': 'allisonthompson@example.com'}]\n"
     ]
    }
   ],
   "source": [
    "from engine.executor import SQLExecutor\n",
    "\n",
    "def test_executor(sql_query):\n",
    "    \"\"\"Test SQLExecutor functionality\"\"\"\n",
    "    executor = SQLExecutor()\n",
    "    \n",
    "    success, results, formatted_results, error = executor.main_executor(sql_query)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nSuccess! Found {len(results)} rows\")\n",
    "        print(\"\\nFormatted Results:\")\n",
    "        print(formatted_results)\n",
    "        print(\"\\nRaw Results:\")\n",
    "        print(results)\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"\\nFailed: {error}\")\n",
    "        return None\n",
    "\n",
    "if refined_sql:\n",
    "    execution_results = test_executor(refined_sql)\n",
    "else:\n",
    "    print(\"Skipping execution as no refined SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results with Mistral...\n",
      "\n",
      "Analysis: 1. Key Findings and Patterns:\n",
      "   - There is only one record in the data that matches the query criteria.\n",
      "   - The learner named 'David Garner' has the email address 'allisonthompson@example.com'. This suggests an unusual naming convention where the learner's name is different from the email address.\n",
      "\n",
      "2. Notable Relationships between Metrics:\n",
      "   - In this case, there is no direct relationship between the name and email metrics as they are separate attributes.\n",
      "\n",
      "3. Important Trends or Anomalies:\n",
      "   - The anomaly in this data is the unusual naming convention where the learner's name is different from the email address.\n",
      "\n",
      "4. Actionable Insights and Recommendations:\n",
      "   - It is recommended to verify the data consistency for this learner's record, as the name and email address do not seem to match the usual naming conventions.\n",
      "   - If this is a common occurrence, it might be beneficial to update the data collection process to allow for more flexible naming conventions or to provide a mechanism for users to specify their preferred name and email.\n",
      "   - Additionally, it could be useful to investigate if there are any other records with similar discrepancies to ensure data quality and consistency.\n"
     ]
    }
   ],
   "source": [
    "from engine.analyzer import SQLAnalyzer\n",
    "\n",
    "def test_analyzer(query, results):\n",
    "    \"\"\"Test SQLAnalyzer functionality with Mistral\"\"\"\n",
    "    analyzer = SQLAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Analyze results using Mistral\n",
    "        print(\"\\nAnalyzing results with Mistral...\")\n",
    "        mistral_results = analyzer.main_analyzer(query, results, llm_model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "        print(\"\\nAnalysis:\", mistral_results['analysis'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if execution_results:\n",
    "    test_analyzer(user_query, execution_results)\n",
    "else:\n",
    "    print(\"Skipping analysis as no execution results were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last chat involved analyzing a dataset containing a single record of a learner with an unusual naming convention, and providing recommendations for data verification and potential updates to the data collection process.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt to test the conversation history\n",
    "prompt = \"What was our last chat about? Explain in a single sentence.\"\n",
    "response = generate_text(prompt, model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
