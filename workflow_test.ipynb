{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Generation and Analysis Workflow Test\n",
    "\n",
    "This notebook demonstrates the complete workflow using multiple LLMs:\n",
    "1. Generating SQL using DeepSeek and Mistral\n",
    "2. Extracting entities from the SQL\n",
    "3. Matching values in the SQL\n",
    "4. Refining the generated SQL\n",
    "5. Executing the refined SQL\n",
    "6. Analyzing the results\n",
    "\n",
    "Each step uses DeepSeek and Mistral for generation, with Gemini as the decision maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config.llm_call import generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference in count between male and female students\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find the name and email of a learner named David Gardner or who has the email allisonthomson@example.com\",\n",
    "    \"Find the number of students form the institution with l4g_code L4G0003\",\n",
    "    \"Show me all learners who have completed their course in the last 6 months\",\n",
    "    \"Analyze the distribution of students by gender, age group, and enrollment year. Skip any 'O' values in data\",\n",
    "    \"What is the difference in count between male and female students\",\n",
    "    \"Order the type of institution in decreasing order of gender 'M', ascending order of gender 'F' and descending order of gender 'O' students\"\n",
    "]\n",
    "\n",
    "# Select which query to test\n",
    "user_query = test_queries[4]\n",
    "\n",
    "print(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQL Generation Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating SQL with Model 1...\n",
      "\n",
      "Generated SQL: SELECT \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'M') - \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'F') AS male_female_count_difference;\n",
      "\n",
      "Generating SQL with Model 2...\n",
      "Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Generated SQL: Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Schema Info: Country (id, name)\n",
      "\n",
      "State (id, name, code, country_code)\n",
      "\n",
      "District (id, name, code, state_code)\n",
      "\n",
      "Institution (id, name, short_name, aicte_code, eamcet_code, l4g_code, l4g_group_code, type, address, website, latlong, district_code)\n",
      "  - type: [Public University, Private University-State, Private University-Deemed to be, Autonomous College, Affiliated College, Unknown]\n",
      "\n",
      "Degree (id, name, short_name)\n",
      "\n",
      "Branch (id, name, short_name, degree_code)\n",
      "\n",
      "Department (id, name, type)\n",
      "  - type: [academic (academic), corporate (corporate), government (government)]\n",
      "\n",
      "Designation (id, name, type, priority)\n",
      "  - type: [academic (academic), corporate (corporate), government (government)]\n",
      "\n",
      "Knowledge_Partner (id, name, address, website, info)\n",
      "\n",
      "Course (id, name, info, knowledge_partner_code)\n",
      "\n",
      "Module (id, name, info, module_sequence_number, theory_practical, duration_minutes, course_code)\n",
      "  - theory_practical: [T (theory), P (practical)]\n",
      "\n",
      "Specialization (id, name, info, knowledge_partner_code)\n",
      "\n",
      "Specialization_Course (id, course_sequence_number, course_code, specialization_code)\n",
      "\n",
      "Program (id, name, info, knowledge_partner_code)\n",
      "\n",
      "Program_Specialization (id, program_code, specialization_code)\n",
      "\n",
      "Program_Requirement (id, name, is_mandatory, program_code)\n",
      "\n",
      "Learner (id, name, email, mobile, gender, date_of_birth, aadhaar_number)\n",
      "  - gender: [M (male), F (female), O (other)]\n",
      "\n",
      "Learner_Education (id, rollno, year_of_joining, year_of_graduation, learner_code, institution_code, branch_code)\n",
      "\n",
      "Learner_Employment (id, empid, year_of_joining, learner_code, institution_code, department_code, designation_code)\n",
      "\n",
      "Learner_Program_Requirement (id, learner_code, program_requirement_code, value)\n",
      "\n",
      "\n",
      "Validator chose Model 1's query\n"
     ]
    }
   ],
   "source": [
    "from engine.generator import SQLGenerator\n",
    "\n",
    "def test_generator(model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"gemini\"):\n",
    "    \"\"\"Test SQL generation functionality with multiple LLMs\"\"\"\n",
    "    generator = SQLGenerator()\n",
    "    \n",
    "    try:\n",
    "        # Generate SQL using Model 1\n",
    "        print(\"\\nGenerating SQL with Model 1...\")\n",
    "        model1_results = generator.main_generator(user_query, llm_model=model1)\n",
    "        print(\"\\nGenerated SQL:\", model1_results['generated_sql'])\n",
    "        \n",
    "        # Generate SQL using Model 2\n",
    "        print(\"\\nGenerating SQL with Model 2...\")\n",
    "        model2_results = generator.main_generator(user_query, llm_model=model2)\n",
    "        print(\"\\nGenerated SQL:\", model2_results['generated_sql'])\n",
    "\n",
    "        print(\"\\nSchema Info:\", model1_results['formatted_metadata'])\n",
    "        \n",
    "        # Use Validator to decide which SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two Generated SQLs for the given query and choose the better one:\n",
    "        \n",
    "        Input Query: {user_query}\n",
    "        \n",
    "        Schema Info: {model1_results['formatted_metadata']}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Generated SQL: {model1_results['generated_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Generated SQL: {model2_results['generated_sql']}\n",
    "        \n",
    "        Consider the input query, schema info, and the generated SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which output is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen query\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's query\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's query\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['generated_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "generated_sql = test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entity Extraction Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting entities with Model 1...\n",
      "\n",
      "Extracted Entities:\n",
      "---\n",
      "Table: Learner\n",
      "Column: gender\n",
      "Value: M\n",
      "---\n",
      "Table: Learner\n",
      "Column: gender\n",
      "Value: F\n",
      "\n",
      "Extracting entities with Model 2...\n",
      "Error: 402 - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "Extracted Entities:\n",
      "\n",
      "Validator chose Model 1's entities\n"
     ]
    }
   ],
   "source": [
    "from engine.entity_extractor import EntityExtractor\n",
    "\n",
    "def test_entity_extractor(sql_query, model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"gemini\"):\n",
    "    \"\"\"Test entity extraction functionality with multiple LLMs\"\"\"\n",
    "    extractor = EntityExtractor()\n",
    "    \n",
    "    try:\n",
    "        # Extract entities using Model 1\n",
    "        print(\"\\nExtracting entities with Model 1...\")\n",
    "        model1_results = extractor.main_entity_extractor(sql_query, llm_model=model1)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model1_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Extract entities using Model 2\n",
    "        print(\"\\nExtracting entities with Model 2...\")\n",
    "        model2_results = extractor.main_entity_extractor(sql_query, llm_model=model2)\n",
    "        print(\"\\nExtracted Entities:\")\n",
    "        for entity in model2_results:\n",
    "            print(\"---\")\n",
    "            print(f\"Table: {entity['table']}\")\n",
    "            print(f\"Column: {entity['column']}\")\n",
    "            print(f\"Value: {entity['value']}\")\n",
    "        \n",
    "        # Use Validator to decide which entities to use\n",
    "        decision_prompt = f\"\"\"Compare these two sets of extracted entities for the input SQL and choose the better one:\n",
    "        \n",
    "        Input SQL: {sql_query}\n",
    "        \n",
    "        Entities 1 (Model 1):\n",
    "        Extracted Entities: {model1_results}\n",
    "        \n",
    "        Entities 2 (Model 2):\n",
    "        Extracted Entities: {model2_results}\n",
    "        \n",
    "        Consider the input SQL and both the extracted entities when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which set is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen entities\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's entities\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's entities\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if generated_sql:\n",
    "    extracted_entities = test_entity_extractor(generated_sql)\n",
    "else:\n",
    "    print(\"Skipping entity extraction as no SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Matching Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Mappings:\n",
      "---\n",
      "Original: 'M'\n",
      "Matched: 'M'\n",
      "Score: 100\n",
      "---\n",
      "Original: 'F'\n",
      "Matched: 'F'\n",
      "Score: 100\n"
     ]
    }
   ],
   "source": [
    "from engine.value_matcher import ValueMatcher\n",
    "\n",
    "def test_value_matcher(extracted_entities):\n",
    "    \"\"\"Test value matching functionality\"\"\"\n",
    "    matcher = ValueMatcher()\n",
    "    \n",
    "    try:\n",
    "        value_mappings = []\n",
    "        for entity in extracted_entities:\n",
    "            match = matcher.main_value_matcher(entity)\n",
    "            value_mappings.extend(match)\n",
    "        \n",
    "        print(\"Value Mappings:\")\n",
    "        for mapping in value_mappings:\n",
    "            print(\"---\")\n",
    "            print(f\"Original: '{mapping['original_value']}'\")\n",
    "            print(f\"Matched: '{mapping['matched_value']}'\")\n",
    "            print(f\"Score: {mapping['score']}\")\n",
    "        \n",
    "        return value_mappings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if extracted_entities:\n",
    "    value_mappings = test_value_matcher(extracted_entities)\n",
    "else:\n",
    "    print(\"Skipping value matching as no entities were extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL Refinement Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refining SQL with Model 1...\n",
      "\n",
      "Refined SQL: SELECT \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'M') - \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'F') AS male_female_count_difference;\n",
      "\n",
      "Refining SQL with Model 2...\n",
      "\n",
      "Refined SQL: SELECT \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'M') - \n",
      "    (SELECT COUNT(*) FROM Learner WHERE gender = 'F') AS male_female_count_difference;\n",
      "\n",
      "Validator chose Model 1's refined SQL\n"
     ]
    }
   ],
   "source": [
    "from engine.refiner import SQLRefiner\n",
    "\n",
    "def test_refiner(sql_query, value_mappings, model1=\"deepseek-chat\", model2=\"mistralai/Mistral-7B-Instruct-v0.3\", validator=\"gemini\"):\n",
    "    \"\"\"Test SQL refinement functionality with multiple LLMs\"\"\"\n",
    "    refiner = SQLRefiner()\n",
    "    \n",
    "    try:\n",
    "        # Refine SQL using Model 1\n",
    "        print(\"\\nRefining SQL with Model 1...\")\n",
    "        model1_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model1)\n",
    "        print(\"\\nRefined SQL:\", model1_results['refined_sql'])\n",
    "        \n",
    "        # Refine SQL using Model 2\n",
    "        print(\"\\nRefining SQL with Model 2...\")\n",
    "        model2_results = refiner.main_refiner(sql_query, value_mappings, llm_model=model2)\n",
    "        print(\"\\nRefined SQL:\", model2_results['refined_sql'])\n",
    "        \n",
    "        # Use Validator to decide which refined SQL to use\n",
    "        decision_prompt = f\"\"\"Compare these two refined SQL queries for the given original SQL and value mappings, and choose the better one:\n",
    "        \n",
    "        Original SQL: {sql_query}\n",
    "        Value Mappings: {value_mappings}\n",
    "        \n",
    "        Query 1 (Model 1):\n",
    "        Refined SQL: {model1_results['refined_sql']}\n",
    "        \n",
    "        Query 2 (Model 2):\n",
    "        Refined SQL: {model2_results['refined_sql']}\n",
    "        \n",
    "        Consider the original sql, value mappings and the refined SQLs when making your decision.\n",
    "        Respond with only '1' or '2' to indicate which query is better.\"\"\"\n",
    "        \n",
    "        decision = generate_text(decision_prompt, model=validator)\n",
    "        \n",
    "        # Use the chosen refined SQL\n",
    "        if decision.strip() == '1':\n",
    "            print(\"\\nValidator chose Model 1's refined SQL\")\n",
    "            chosen_results = model1_results\n",
    "        else:\n",
    "            print(\"\\nValidator chose Model 2's refined SQL\")\n",
    "            chosen_results = model2_results\n",
    "        \n",
    "        return chosen_results['refined_sql']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if value_mappings:\n",
    "    refined_sql = test_refiner(generated_sql, value_mappings)\n",
    "else:\n",
    "    print(\"Skipping refinement as no value mappings were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Execution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Found 1 rows\n",
      "\n",
      "Formatted Results:\n",
      "male_female_count_difference\n",
      "----------------------------\n",
      "21                          \n",
      "\n",
      "Raw Results:\n",
      "[{'male_female_count_difference': 21}]\n"
     ]
    }
   ],
   "source": [
    "from engine.executor import SQLExecutor\n",
    "\n",
    "def test_executor(sql_query):\n",
    "    \"\"\"Test SQLExecutor functionality\"\"\"\n",
    "    executor = SQLExecutor()\n",
    "    \n",
    "    success, results, formatted_results, error = executor.main_executor(sql_query)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nSuccess! Found {len(results)} rows\")\n",
    "        print(\"\\nFormatted Results:\")\n",
    "        print(formatted_results)\n",
    "        print(\"\\nRaw Results:\")\n",
    "        print(results)\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"\\nFailed: {error}\")\n",
    "        return None\n",
    "\n",
    "if refined_sql:\n",
    "    execution_results = test_executor(refined_sql)\n",
    "else:\n",
    "    print(\"Skipping execution as no refined SQL was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis Test with Multiple LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results with DeepSeek...\n",
      "\n",
      "Analysis: Here's a comprehensive analysis of the data for the query \"What is the difference in count between male and female students\":\n",
      "\n",
      "1. Key Findings:\n",
      "- There are 21 more male students than female students in the dataset\n",
      "- The gender disparity is significant enough to warrant attention (21 students difference)\n",
      "\n",
      "2. Notable Relationships:\n",
      "- The single metric shows a clear gender imbalance in student population\n",
      "- No other demographic breakdowns are available to analyze intersectional factors\n",
      "\n",
      "3. Trends/Anomalies:\n",
      "- The data shows a consistent pattern of male overrepresentation\n",
      "- Without historical data, we cannot determine if this is an improving/worsening trend\n",
      "\n",
      "4. Actionable Insights & Recommendations:\n",
      "- Investigate root causes of gender disparity (admissions processes, institutional factors)\n",
      "- Implement targeted outreach programs to attract more female applicants\n",
      "- Consider gender-balanced admissions policies if appropriate for the institution\n",
      "- Collect additional demographic data to understand intersectional impacts\n",
      "- Monitor this metric over time to measure effectiveness of interventions\n",
      "- Conduct student satisfaction surveys by gender to identify potential equity issues\n",
      "\n",
      "The analysis suggests a need for proactive measures to address the gender imbalance, while recognizing that more detailed data would enable more targeted solutions.\n"
     ]
    }
   ],
   "source": [
    "from engine.analyzer import SQLAnalyzer\n",
    "\n",
    "def test_analyzer(query, results):\n",
    "    \"\"\"Test SQLAnalyzer functionality with DeepSeek\"\"\"\n",
    "    analyzer = SQLAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Analyze results using DeepSeek\n",
    "        print(\"\\nAnalyzing results with DeepSeek...\")\n",
    "        mistral_results = analyzer.main_analyzer(query, results, llm_model=\"deepseek-chat\")\n",
    "        print(\"\\nAnalysis:\", mistral_results['analysis'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if execution_results:\n",
    "    test_analyzer(user_query, execution_results)\n",
    "else:\n",
    "    print(\"Skipping analysis as no execution results were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our last chat was about analyzing the difference in count between male and female students from a dataset showing 21 more males than females.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt to test the conversation history\n",
    "prompt = \"What was our last chat about? Explain in a single sentence.\"\n",
    "response = generate_text(prompt, model=\"deepseek-chat\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
